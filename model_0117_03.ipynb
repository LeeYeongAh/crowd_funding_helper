{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Modeling finished\n",
      "Word2Vec Modeling finished\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from gensim.models import word2vec\n",
    "from smart_open import smart_open\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "maxparse = sys.maxsize\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(maxparse)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxparse = int(maxparse/10)\n",
    "\n",
    " #네이버 영화 코퍼스를 읽는다.\n",
    "f = open('C://Users//vivid//바탕 화면//word2vec_03//crowd_funding_helper//mix_oktorigin_1_adddata.txt', 'r', encoding='utf-8')\n",
    "rdr = csv.reader(f, delimiter='\\t')\n",
    "rdw = list(rdr)\n",
    "f.close()\n",
    "\n",
    " #트위터 형태소 분석기를 로드한다. Twiter가 KoNLPy v0.4.5 부터 Okt로 변경 되었다.\n",
    "twitter = Okt()\n",
    "\n",
    " #텍스트를 한줄씩 처리합니다.\n",
    "result = []\n",
    "for line in rdw:\n",
    "     #형태소 분석하기, 단어 기본형 사용\n",
    "    malist = twitter.pos( line[0], norm=True, stem=True)\n",
    "    r = []\n",
    "    for word in malist:\n",
    "         #Josa”, “Eomi”, “'Punctuation” 는 제외하고 처리\n",
    "        if not word[1] in [\"Josa\",\"Eomi\",\"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "     #형태소 사이에 공백 \" \"  을 넣습니다. 그리고 양쪽 공백을 지웁니다.\n",
    "    rl = (\" \".join(r)).strip()\n",
    "    result.append(rl)\n",
    "     #print(rl)\n",
    "\n",
    " #형태소들을 별도의 파일로 저장 합니다.\n",
    "with open(\"embedding//mix_adddata_0117_02.nlp\",'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(result))\n",
    "\n",
    " #Word2Vec 모델 만들기\n",
    "wData = word2vec.LineSentence(\"embedding//mix_adddata_0117_02.nlp\")\n",
    "wModel =word2vec.Word2Vec(wData, size=200, window=10, hs=1, min_count=2, sg=1)\n",
    "wModel.save(\"embedding//mix_addata_0117_02.model\")\n",
    "print(\"Word2Vec Modeling finished\")\n",
    "\n",
    "with open(\"embedding//mix_adddata_0117_03.nlp\",'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(result))\n",
    "#임베딩 차원수 늘리기.\n",
    "wData2 = word2vec.LineSentence(\"embedding//mix_adddata_0117_03.nlp\")\n",
    "wModel2 =word2vec.Word2Vec(wData, size=500, window=10, hs=1, min_count=2, sg=1)\n",
    "wModel2.save(\"embedding//mix_addata_0117_03.model\")\n",
    "\n",
    "print(\"Word2Vec Modeling finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Modeling Size 100 finished\n",
      "Word2Vec Modeling Size 200finished\n",
      "Word2Vec Modeling Size 300finished\n",
      "Word2Vec Modeling Size 400finished\n",
      "Word2Vec Modeling Size 500finished\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from gensim.models import word2vec\n",
    "from smart_open import smart_open\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "maxparse = sys.maxsize\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(maxparse)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxparse = int(maxparse/10)\n",
    "\n",
    " #네이버 영화 코퍼스를 읽는다.\n",
    "f = open('C://Users//vivid//바탕 화면//word2vec_03//crowd_funding_helper//mix_oktorigin_1_adddata.txt', 'r', encoding='utf-8')\n",
    "rdr = csv.reader(f, delimiter='\\t')\n",
    "rdw = list(rdr)\n",
    "f.close()\n",
    "\n",
    " #트위터 형태소 분석기를 로드한다. Twiter가 KoNLPy v0.4.5 부터 Okt로 변경 되었다.\n",
    "twitter = Okt()\n",
    "\n",
    " #텍스트를 한줄씩 처리합니다.\n",
    "result = []\n",
    "for line in rdw:\n",
    "     #형태소 분석하기, 단어 기본형 사용\n",
    "    malist = twitter.pos( line[0], norm=True, stem=True)\n",
    "    r = []\n",
    "    for word in malist:\n",
    "         #Josa”, “Eomi”, “'Punctuation” 는 제외하고 처리\n",
    "        if not word[1] in [\"Josa\",\"Eomi\",\"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "     #형태소 사이에 공백 \" \"  을 넣습니다. 그리고 양쪽 공백을 지웁니다.\n",
    "    rl = (\" \".join(r)).strip()\n",
    "    result.append(rl)\n",
    "     #print(rl)\n",
    "\n",
    " #형태소들을 별도의 파일로 저장 합니다.\n",
    "with open(\"embedding//0117_size100.nlp\",'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(result))\n",
    "\n",
    " #Word2Vec 모델 만들기\n",
    "wData = word2vec.LineSentence(\"embedding//0117_size100.nlp\")\n",
    "wModel =word2vec.Word2Vec(wData, size=100, window=10, hs=1, min_count=2, sg=1)\n",
    "wModel.save(\"embedding//0117_size100.model\")\n",
    "print(\"Word2Vec Modeling Size 100 finished\")\n",
    "\n",
    "#임베딩 차원수 늘리기.\n",
    "with open(\"embedding//0117_size200.nlp\",'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(result))\n",
    "wData2 = word2vec.LineSentence(\"embedding//0117_size200.nlp\")\n",
    "wModel2 =word2vec.Word2Vec(wData, size=500, window=10, hs=1, min_count=2, sg=1)\n",
    "wModel2.save(\"embedding//0117_size200.model\")\n",
    "\n",
    "print(\"Word2Vec Modeling Size 200finished\")\n",
    "\n",
    "\n",
    "with open(\"embedding//0117_size300.nlp\",'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(result))\n",
    "wData2 = word2vec.LineSentence(\"embedding//0117_size300.nlp\")\n",
    "wModel2 =word2vec.Word2Vec(wData, size=500, window=10, hs=1, min_count=2, sg=1)\n",
    "wModel2.save(\"embedding//0117_size300.model\")\n",
    "\n",
    "print(\"Word2Vec Modeling Size 300finished\")\n",
    "\n",
    "\n",
    "with open(\"embedding//0117_size400.nlp\",'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(result))\n",
    "wData2 = word2vec.LineSentence(\"embedding//0117_size400.nlp\")\n",
    "wModel2 =word2vec.Word2Vec(wData, size=500, window=10, hs=1, min_count=2, sg=1)\n",
    "wModel2.save(\"embedding//0117_size400.model\")\n",
    "\n",
    "print(\"Word2Vec Modeling Size 400finished\")\n",
    "\n",
    "\n",
    "with open(\"embedding//0117_size500.nlp\",'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(result))\n",
    "wData2 = word2vec.LineSentence(\"embedding//0117_size500.nlp\")\n",
    "wModel2 =word2vec.Word2Vec(wData, size=500, window=10, hs=1, min_count=2, sg=1)\n",
    "wModel2.save(\"embedding//0117_size500.model\")\n",
    "\n",
    "print(\"Word2Vec Modeling Size 500finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
